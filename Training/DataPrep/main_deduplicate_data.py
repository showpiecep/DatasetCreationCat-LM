import gzip
import multiprocessing
import os
import shutil
import hashlib
import time
import numpy as np
import re
import sys
import json
from multiprocessing import Pool
from tqdm import tqdm
import shutil
from tqdm import tqdm
import argparse


def get_args():
    parser = argparse.ArgumentParser()

    parser.add_argument("--repo_dir", default='/data/GitHubMining/CurrentState/')
    parser.add_argument("--copy_dir", default='/data/GitHubMining/CurrentStateDeduplicated/')
    parser.add_argument("--metadata_dir", default='/data/GitHubMining/DeduplicationMetaData/')
    parser.add_argument("--repos_file_path", default='../GitHubMining/python-top-repos.txt')
    parser.add_argument("--pl", default='python')

    return parser.parse_args()


config = {
    "line_max": 1000,
    "line_mean": 100,
    "alphaneumeric_frac": 0.25,
    "min_file_len": 100,
    "max_file_len": 1_000_000
}

PATTERN = re.compile(r'\s+')

def hash_func(text):
    """
    Возвращает строку, содержащую MD5-хэш исходного текста без пробельных символов.
    """
    return hashlib.md5(re.sub(PATTERN, '', text).encode("utf-8")).hexdigest()

def get_hash(example):
    """Get hash of content field."""
    return hash_func(example)

def line_stats(example):
    """Calculates mean and max line length of file."""
    line_lengths = [len(line) for line in example.splitlines()]
    return np.mean(line_lengths), max(line_lengths)


def alpha_stats(example):
    """Calculates mean and max line length of file."""
    alpha_frac = np.mean([c.isalnum() for c in example])
    return alpha_frac


def is_autogenerated(example, scan_width=5):
    """Check if file is autogenerated by looking for keywords in the first few lines of the file."""
    keywords = ["auto-generated", "autogenerated", "automatically generated"]
    lines = example.splitlines()
    for _, line in zip(range(scan_width), lines):
        for keyword in keywords:
            if keyword in line.lower():
                return True
    else:
        return False


def filter_projects(repo_dir, copy_dir,  pl, org, project, hashes_map):
    base_dir = os.path.join(repo_dir, pl, org, project)
    copy_dir = os.path.join(copy_dir, pl, org, project)
    
    removed_files = []
    for path, subdirs, files in os.walk(base_dir):
        for name in files:
            try:
                with open(f"{path}/{name}", errors='ignore') as f:
                    file_content = f.read()
                    num_chars = len(file_content)
                    
                    if num_chars < config["min_file_len"]:
                        print(f"{path}/{name} chars too short")
                        removed_files.append({"filename": f"{path}/{name}", "reason": "chars_too_short"})
                        continue
                    
                    if num_chars > config["max_file_len"]:
                        print(f"{path}/{name} chars too long")
                        removed_files.append({"filename": f"{path}/{name}", "reason": "chars_too_long"})
                        continue

                    file_hash = get_hash(file_content)
                    mean_len, max_len = line_stats(file_content)
                    alpha_frac = alpha_stats(file_content)
                    is_autogenerated_val = is_autogenerated(file_content)
                    
                    copy_path = path.replace(base_dir, copy_dir)
                    
                    if alpha_frac < config["alphaneumeric_frac"]:
                        print(f"{path}/{name} alphanum too few")
                        removed_files.append({"filename": f"{path}/{name}", "reason": "alphaneumeric_too_few"})
                        continue

                    if mean_len > config["line_mean"]:
                        print(f"{path}/{name} line mean too many")
                        removed_files.append({"filename": f"{path}/{name}", "reason": "line_len_too_long_mean"})
                        continue

                    if max_len > config["line_max"]:
                        print(f"{path}/{name} line max too many")
                        removed_files.append({"filename": f"{path}/{name}", "reason": "line_len_too_long_max"})
                        continue

                    if is_autogenerated_val:
                        print(f"{path}/{name} autogenerated")
                        removed_files.append({"filename": f"{path}/{name}", "reason": "is_autogenerated"})
                        continue

                    if file_hash not in hashes_map:
                        hashes_map[file_hash] = []
                    
                    hashes_map[file_hash].append(f"{copy_path}/{name}")

                    if len(hashes_map[file_hash]) > 1:
                        print(f"{path}/{name} duplicate")
                        removed_files.append({"filename": f"{path}/{name}", "reason": "duplicate_hash"})
                        continue

                    os.makedirs(copy_path, exist_ok=True)
                    shutil.copyfile(f"{path}/{name}", f"{copy_path}/{name}")
            except Exception as e:
                print(f"Unable to open file {path}/{name}")
                print(e)


    return removed_files


if __name__ == '__main__':
    # repo_dir = sys.argv[1] if len(sys.argv) > 1 else '/data/GitHubMining/CurrentState/'
    # copy_dir = sys.argv[2] if len(sys.argv) > 2 else '/data/GitHubMining/CurrentStateDeduplicated/'
    # metadata_dir = sys.argv[3] if len(sys.argv) > 3 else '/data/GitHubMining/DeduplicationMetaData/'
    # repos_file_path = sys.argv[4] if len(sys.argv) > 4 else '../GitHubMining/python-top-repos.txt'
    # pl = sys.argv[5] if len(sys.argv) > 5 else 'python'
    args = get_args()
    
    hashes_map = {}
    removed_files = {}

    with open(args.repos_file_path) as f:
        repo_links_list = f.read().split('\n')
    
    for project_link in tqdm(repo_links_list):
        project_link = project_link.split('\t')[0]
        if project_link.endswith('/'):
            project_link = project_link[:-1]
        *_, org, project = project_link.split('/')
        #print(f"Processing {pl}/{org}/{project}")

        removed_files[f"{org}/{project}"] = filter_projects(args.repo_dir, args.copy_dir, args.pl, org, project, hashes_map)
    
    
        os.makedirs(f"{args.metadata_dir}/{args.pl}", exist_ok=True)
        with open(f"{args.metadata_dir}/{args.pl}/removed_files.json", "w") as f:
            json.dump(removed_files, f)
        
        with open(f"{args.metadata_dir}/{args.pl}/hashes_map.json", "w") as f:
            json.dump(hashes_map, f)

